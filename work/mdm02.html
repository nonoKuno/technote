<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>マスク化拡散モデル解説 第2回：マスキングパラダイム - 双方向コンテキストの力</title>
    <style>
        body { 
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.8; 
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 { 
            color: #1a1a1a;
            border-bottom: 2px solid #f0f0f0;
            padding-bottom: 10px;
        }
        h1 {
            font-size: 2.5em;
        }
        h2 {
            font-size: 2em;
            margin-top: 40px;
        }
        h3 {
            font-size: 1.5em;
            border-bottom: 1px solid #f0f0f0;
            margin-top: 30px;
        }
        code { 
            background-color: #f4f4f4; 
            padding: 2px 6px; 
            border-radius: 4px; 
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            font-size: 0.9em;
        }
        blockquote { 
            border-left: 5px solid #ccc; 
            padding-left: 20px; 
            margin-left: 0;
            background-color: #f9f9f9;
            padding-top: 10px;
            padding-bottom: 10px;
        }
     .container {
            padding: 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>マスク化拡散モデル解説 第2回：マスキングパラダイム - 双方向コンテキストの力</h1>
        
        <h2>前回の振り返り</h2>
        <p>
            前回の記事では、マスク化拡散モデル（MDM）を支える一つ目の柱である「拡散パラダイム」について解説した。具体的には、Denoising Diffusion Probabilistic Models (DDPM) が、データにノイズを付加する順方向プロセスと、ノイズからデータを復元する逆方向プロセスによって高品質な生成を実現する仕組みを説明した。また、このプロセスが確率微分方程式（SDE）によって統一的に記述できることにも触れた。
        </p>
        <p>
            本稿では、MDMのもう一つの重要な柱である「マスキングパラダイム」に焦点を当てる。このパラダイムは、特に自然言語処理の分野で大きなブレークスルーをもたらしたBERTによって確立されたものである。
        </p>

        <h2>第2章 マスキングパラダイム：双方向コンテキストの獲得</h2>

        <h3>2.1. BERTとマスク化言語モデル (MLM)</h3>
        <p>
            MDMのもう一つの基礎となるのは、Devlinら (2018) の論文「BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding」で提案されたマスク化言語モデル（Masked Language Model, MLM）である [1, 2, 3, 4, 5, 6, 7, 8, 9]。
        </p>
        <p>
            <strong>一方向性の制約:</strong> BERT以前の主要な言語モデル（例：GPT）は、一方向性（通常は左から右）であり、シーケンスの次の単語を予測するように訓練されていた。このアーキテクチャは強力であるものの、単語の左右両方の文脈を同時に深く考慮する能力には構造的な制約があった [1, 2]。多くの自然言語理解タスクでは、文全体の双方向的な文脈理解が不可欠である。
        </p>
        <p>
            <strong>MLM目的関数:</strong> BERTはこの制約を、MLMという事前学習タスクによって克服した。MLMは、次の単語を予測する代わりに、入力文の一部を隠し（マスクし）、モデルにその隠された単語を周囲の文脈全体（左と右の両方）から推測させる [1, 2, 3, 6]。
        </p>
        <blockquote>
            <strong>MLMの具体的なプロセス：</strong>
            <ol>
                <li>入力トークンシーケンスの中からランダムに15%のトークンを選択する。</li>
                <li>選択されたトークンに対し、以下の処理を行う：
                    <ul>
                        <li>80%の確率で、特別な<code></code>トークンに置き換える。</li>
                        <li>10%の確率で、語彙内のランダムな別のトークンに置き換える。</li>
                        <li>10%の確率で、元のトークンのまま変更しない。</li>
                    </ul>
                </li>
                <li>モデルは、この「破損」した入力シーケンスから、マスクされた位置の元のトークンを予測するように訓練される。</li>
            </ol>
        </blockquote>
        <p>
            この置き換え戦略は、モデルが単に<code></code>トークンの位置を予測するだけでなく、入力表現そのものが正しいかどうかも含めて学習することを促し、より汎用性の高い文脈表現の獲得に寄与する [3]。
        </p>
        <p>
            さらにBERTは、MLMと並行して「次文予測（Next Sentence Prediction, NSP）」というタスクでも事前学習される。これは、与えられた二つの文が原文において連続しているかどうかを判定する二値分類タスクであり、モデルが文と文の間の関係性を理解する能力を養うことを目的としている [2, 3, 9]。
        </p>
        
        <h3>デノイジング概念の統合</h3>
        <p>
            MDMの核心的な着想は、DDPMにおける「デノイジング」（連続値であるピクセルからガウスノイズを除去する操作）と、BERTのMLMにおける「デノイジング」（離散値であるテキストから<code></code>トークンを復元する操作）が、本質的に同じ「破損からの復元」というタスクであると見なした点にある。MDMは、DDPMの<strong>反復的な生成プロセス</strong>と、BERTの<strong>離散データに対するデノイジング目的関数</strong>を直接的に統合したものである。
        </p>
        <p>
            両者のプロセスを比較すると、この関係はより明確になる。
        </p>
        <ul>
            <li><strong>DDPM:</strong> 連続空間におけるデノイジングステップであり、$p(x_{t-1} | x_t)$を学習する。ノイズが付加されたデータ$x_t$から、よりノイズの少ないデータ$x_{t-1}$を予測する。</li>
            <li><strong>BERT (MLM):</strong> 離散空間におけるデノイジングステップであり、$p(\text{masked\_token} | \text{unmasked\_context})$を学習する。一部がマスクされた「破損」シーケンスから、元の「クリーンな」トークンを予測する。</li>
            <li><strong>MDM:</strong> BERTのマスキング操作を、DDPMのような反復的なプロセスとして適用する。順方向プロセスは段階的なマスキングと見なせ、逆方向プロセスは反復的なアンマスキング（MLMタスクの遂行）となる。</li>
        </ul>
        <p>
            したがって、MDMのフレームワークは、これら二つのパラダイムから単に影響を受けただけでなく、一方のプロセスと他方の目的関数を直接的に組み合わせたものと理解できる。この点が、MDMが自然かつ強力なモデルである理由を説明する鍵となる。
        </p>
        
        <hr style="margin: 40px 0;">
        <p>
            本稿では、マスク化拡散モデル（MDM）の基礎となる二つ目の柱、マスク化言語モデルについて解説した。ここまでで、MDMを支える「拡散」と「マスキング」という二つの概念が出揃った。次回の記事では、これら二つのアイデアがどのように融合して現代のMDMフレームワークが形成され、その後の理論的な単純化が性能向上にどう繋がったかを探求する。
        </p>
    </div>
</body>
</html>